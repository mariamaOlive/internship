{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "model = YOLO('best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)  # 0 is the default webcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each frame\n",
    "def process_frame(frame):\n",
    "    # Reduce the size of the frame for faster processing\n",
    "    resized_frame = cv2.resize(frame, (640, 480))\n",
    "    \n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(resized_frame)\n",
    "    \n",
    "    # Draw the results on the frame\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()  # xyxy (N, 4)\n",
    "        confs = result.boxes.conf.cpu().numpy()  # (N,)\n",
    "        class_ids = result.boxes.cls.cpu().numpy()  # (N,)\n",
    "        for box, conf, class_id in zip(boxes, confs, class_ids):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            label = model.names[int(class_id)]  # Get the class name\n",
    "            cv2.rectangle(resized_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(resized_frame, f'{label} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    return resized_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1440x1920 1 Mask, 1 No-Helmet, 1103.3ms\n",
      "Speed: 34.2ms preprocess, 1103.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Mask, 1 No-Helmet, 1054.1ms\n",
      "Speed: 20.3ms preprocess, 1054.1ms inference, 2.5ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1048.7ms\n",
      "Speed: 19.8ms preprocess, 1048.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1049.6ms\n",
      "Speed: 22.0ms preprocess, 1049.6ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1051.1ms\n",
      "Speed: 19.5ms preprocess, 1051.1ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Mask, 1 No-Helmet, 1052.9ms\n",
      "Speed: 20.3ms preprocess, 1052.9ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1050.9ms\n",
      "Speed: 19.3ms preprocess, 1050.9ms inference, 2.5ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 1046.8ms\n",
      "Speed: 20.8ms preprocess, 1046.8ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1055.6ms\n",
      "Speed: 19.6ms preprocess, 1055.6ms inference, 2.5ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1070.2ms\n",
      "Speed: 21.7ms preprocess, 1070.2ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1043.7ms\n",
      "Speed: 19.5ms preprocess, 1043.7ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1057.4ms\n",
      "Speed: 20.0ms preprocess, 1057.4ms inference, 2.5ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Mask, 1 No-Helmet, 1049.3ms\n",
      "Speed: 20.1ms preprocess, 1049.3ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1034.2ms\n",
      "Speed: 19.9ms preprocess, 1034.2ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Mask, 1 No-Helmet, 1029.6ms\n",
      "Speed: 22.6ms preprocess, 1029.6ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Mask, 1 No-Helmet, 1019.7ms\n",
      "Speed: 21.3ms preprocess, 1019.7ms inference, 2.5ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Mask, 1 No-Helmet, 1009.6ms\n",
      "Speed: 20.8ms preprocess, 1009.6ms inference, 2.5ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Mask, 1 No-Helmet, 1016.2ms\n",
      "Speed: 18.7ms preprocess, 1016.2ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1052.4ms\n",
      "Speed: 18.9ms preprocess, 1052.4ms inference, 2.5ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Mask, 1 No-Helmet, 1000.9ms\n",
      "Speed: 19.7ms preprocess, 1000.9ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 981.5ms\n",
      "Speed: 20.5ms preprocess, 981.5ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 987.4ms\n",
      "Speed: 18.8ms preprocess, 987.4ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 978.9ms\n",
      "Speed: 18.7ms preprocess, 978.9ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 971.6ms\n",
      "Speed: 20.7ms preprocess, 971.6ms inference, 2.5ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 977.0ms\n",
      "Speed: 19.7ms preprocess, 977.0ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 977.1ms\n",
      "Speed: 19.5ms preprocess, 977.1ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 970.7ms\n",
      "Speed: 19.1ms preprocess, 970.7ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 965.8ms\n",
      "Speed: 20.8ms preprocess, 965.8ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 994.0ms\n",
      "Speed: 21.5ms preprocess, 994.0ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 962.0ms\n",
      "Speed: 21.4ms preprocess, 962.0ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 963.8ms\n",
      "Speed: 21.2ms preprocess, 963.8ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 955.3ms\n",
      "Speed: 21.2ms preprocess, 955.3ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 972.0ms\n",
      "Speed: 21.1ms preprocess, 972.0ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 956.9ms\n",
      "Speed: 20.0ms preprocess, 956.9ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 967.7ms\n",
      "Speed: 20.8ms preprocess, 967.7ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 971.9ms\n",
      "Speed: 21.0ms preprocess, 971.9ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 969.8ms\n",
      "Speed: 20.2ms preprocess, 969.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 961.3ms\n",
      "Speed: 18.6ms preprocess, 961.3ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 971.5ms\n",
      "Speed: 21.0ms preprocess, 971.5ms inference, 2.1ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 948.0ms\n",
      "Speed: 21.1ms preprocess, 948.0ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 963.0ms\n",
      "Speed: 21.2ms preprocess, 963.0ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 952.6ms\n",
      "Speed: 20.6ms preprocess, 952.6ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 979.7ms\n",
      "Speed: 21.4ms preprocess, 979.7ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 960.5ms\n",
      "Speed: 19.4ms preprocess, 960.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 994.3ms\n",
      "Speed: 20.5ms preprocess, 994.3ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 959.7ms\n",
      "Speed: 21.0ms preprocess, 959.7ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 964.3ms\n",
      "Speed: 21.0ms preprocess, 964.3ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 956.1ms\n",
      "Speed: 21.1ms preprocess, 956.1ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 959.7ms\n",
      "Speed: 21.0ms preprocess, 959.7ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 955.1ms\n",
      "Speed: 20.7ms preprocess, 955.1ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 981.2ms\n",
      "Speed: 21.0ms preprocess, 981.2ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 958.8ms\n",
      "Speed: 18.9ms preprocess, 958.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 960.7ms\n",
      "Speed: 20.7ms preprocess, 960.7ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 958.7ms\n",
      "Speed: 20.7ms preprocess, 958.7ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 969.6ms\n",
      "Speed: 21.0ms preprocess, 969.6ms inference, 2.1ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 952.8ms\n",
      "Speed: 19.6ms preprocess, 952.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 957.1ms\n",
      "Speed: 20.6ms preprocess, 957.1ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 947.6ms\n",
      "Speed: 21.0ms preprocess, 947.6ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 963.6ms\n",
      "Speed: 18.9ms preprocess, 963.6ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 950.0ms\n",
      "Speed: 21.0ms preprocess, 950.0ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 958.7ms\n",
      "Speed: 20.8ms preprocess, 958.7ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 968.6ms\n",
      "Speed: 20.9ms preprocess, 968.6ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 965.2ms\n",
      "Speed: 19.3ms preprocess, 965.2ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 950.3ms\n",
      "Speed: 21.0ms preprocess, 950.3ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 974.1ms\n",
      "Speed: 21.5ms preprocess, 974.1ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 951.9ms\n",
      "Speed: 19.4ms preprocess, 951.9ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 957.5ms\n",
      "Speed: 19.6ms preprocess, 957.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 958.1ms\n",
      "Speed: 20.6ms preprocess, 958.1ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 959.3ms\n",
      "Speed: 20.8ms preprocess, 959.3ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 946.8ms\n",
      "Speed: 20.9ms preprocess, 946.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 969.1ms\n",
      "Speed: 18.4ms preprocess, 969.1ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 950.8ms\n",
      "Speed: 20.8ms preprocess, 950.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 967.8ms\n",
      "Speed: 21.0ms preprocess, 967.8ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 949.5ms\n",
      "Speed: 20.8ms preprocess, 949.5ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 960.7ms\n",
      "Speed: 21.1ms preprocess, 960.7ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 947.9ms\n",
      "Speed: 21.5ms preprocess, 947.9ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 962.6ms\n",
      "Speed: 20.7ms preprocess, 962.6ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 957.8ms\n",
      "Speed: 20.8ms preprocess, 957.8ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 954.9ms\n",
      "Speed: 20.8ms preprocess, 954.9ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 974.6ms\n",
      "Speed: 21.0ms preprocess, 974.6ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 966.4ms\n",
      "Speed: 21.0ms preprocess, 966.4ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 950.5ms\n",
      "Speed: 21.0ms preprocess, 950.5ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 957.5ms\n",
      "Speed: 20.8ms preprocess, 957.5ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 963.0ms\n",
      "Speed: 20.8ms preprocess, 963.0ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 954.6ms\n",
      "Speed: 19.3ms preprocess, 954.6ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 952.6ms\n",
      "Speed: 20.7ms preprocess, 952.6ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 1 No-Mask, 956.0ms\n",
      "Speed: 20.7ms preprocess, 956.0ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 950.2ms\n",
      "Speed: 21.6ms preprocess, 950.2ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 955.6ms\n",
      "Speed: 20.7ms preprocess, 955.6ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 948.5ms\n",
      "Speed: 21.0ms preprocess, 948.5ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 969.6ms\n",
      "Speed: 18.8ms preprocess, 969.6ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 950.6ms\n",
      "Speed: 20.5ms preprocess, 950.6ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 966.1ms\n",
      "Speed: 20.7ms preprocess, 966.1ms inference, 1.8ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 Helmet, 963.7ms\n",
      "Speed: 21.0ms preprocess, 963.7ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 966.4ms\n",
      "Speed: 20.5ms preprocess, 966.4ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 953.5ms\n",
      "Speed: 20.6ms preprocess, 953.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 952.6ms\n",
      "Speed: 26.4ms preprocess, 952.6ms inference, 2.1ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 965.6ms\n",
      "Speed: 18.6ms preprocess, 965.6ms inference, 1.9ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 2 No-Masks, 994.9ms\n",
      "Speed: 20.9ms preprocess, 994.9ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1 No-Mask, 974.6ms\n",
      "Speed: 21.2ms preprocess, 974.6ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1 No-Mask, 978.7ms\n",
      "Speed: 20.8ms preprocess, 978.7ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 980.9ms\n",
      "Speed: 19.5ms preprocess, 980.9ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1039.2ms\n",
      "Speed: 19.9ms preprocess, 1039.2ms inference, 2.7ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1 No-Mask, 1012.2ms\n",
      "Speed: 21.1ms preprocess, 1012.2ms inference, 2.4ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1 No-Mask, 1060.7ms\n",
      "Speed: 19.8ms preprocess, 1060.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 1020.3ms\n",
      "Speed: 20.4ms preprocess, 1020.3ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1 No-Mask, 987.8ms\n",
      "Speed: 21.2ms preprocess, 987.8ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 960.0ms\n",
      "Speed: 20.7ms preprocess, 960.0ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Glove, 960.8ms\n",
      "Speed: 20.8ms preprocess, 960.8ms inference, 2.2ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 954.8ms\n",
      "Speed: 21.7ms preprocess, 954.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 967.9ms\n",
      "Speed: 20.1ms preprocess, 967.9ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 (no detections), 946.7ms\n",
      "Speed: 22.1ms preprocess, 946.7ms inference, 2.0ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1 No-Mask, 1001.6ms\n",
      "Speed: 19.3ms preprocess, 1001.6ms inference, 2.3ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Helmet, 1 No-Mask, 1055.7ms\n",
      "Speed: 20.6ms preprocess, 1055.7ms inference, 2.5ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 1060.7ms\n",
      "Speed: 20.2ms preprocess, 1060.7ms inference, 2.6ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 1070.9ms\n",
      "Speed: 20.0ms preprocess, 1070.9ms inference, 3.5ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n",
      "0: 1440x1920 1 No-Mask, 1073.0ms\n",
      "Speed: 20.5ms preprocess, 1073.0ms inference, 2.7ms postprocess per image at shape (1, 3, 1440, 1920)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Process the frame\u001b[39;00m\n\u001b[1;32m     19\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 20\u001b[0m processed_frame \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate and display the frame processing time\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mprocess_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      4\u001b[0m resized_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run YOLOv8 inference on the frame\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Draw the results on the frame\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/engine/model.py:174\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/engine/model.py:442\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/engine/predictor.py:254\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/engine/predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/nn/autobackend.py:456\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 456\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/nn/tasks.py:102\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/nn/tasks.py:120\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/nn/tasks.py:141\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 141\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    142\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/nn/modules/head.py:57\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_end2end(x)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl):\n\u001b[0;32m---> 57\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3[i](x[i])), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/EDISS_Resources/Internships/Aptar/yolov8-env/lib/python3.8/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process the video and display the results\n",
    "import time\n",
    "\n",
    "try:\n",
    "    frame_skip = 1  # Process every 2nd frame to reduce load\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % frame_skip != 0:\n",
    "            continue\n",
    "        \n",
    "        # Process the frame\n",
    "        start_time = time.time()\n",
    "        processed_frame = process_frame(frame)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate and display the frame processing time\n",
    "        fps = 1 / (end_time - start_time)\n",
    "        cv2.putText(processed_frame, f'FPS: {fps:.2f}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('YOLOv8 Video Test', processed_frame)\n",
    "        \n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # Release the video capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Release the webcam and close windows if not already done\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
